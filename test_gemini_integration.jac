"""AI Task Manager - Gemini API Integration Test
Simplified version to verify byLLM integration is working correctly"""

import from byllm { Model }
import random;

# Global model configuration
glob llm = Model(model_name="gemini/gemini-2.0-flash");

# Define task categories as enum for type safety
enum TaskCategory {
    WORK,
    PERSONAL, 
    HEALTH,
    LEARNING,
    GENERAL
}

"""Analyze task description and intelligently categorize it using AI"""
def categorize_task_ai(description: str) -> TaskCategory by llm();

"""Generate a simple AI greeting to test connectivity"""
def test_ai_connection() -> str by llm();

with entry:__main__ {
    print("ğŸ§ª Testing Real Gemini API Integration");
    print("=" * 40);
    
    try {
        print("ğŸ“¡ Testing AI connection...");
        greeting = test_ai_connection();
        print(f"âœ… AI Response: {greeting}");
        
        print("\nğŸ“ Testing task categorization...");
        category = categorize_task_ai("finish quarterly report by friday");
        print(f"âœ… Task Category: {category.name}");
        
        print(f"\nğŸ‰ SUCCESS: Real Gemini API integration is working!");
        print(f"ğŸš€ byLLM framework successfully connected to Gemini");
        
    } except Exception as e {
        print(f"âš ï¸ API Issue: {e}");
        
        # Check for specific error types
        error_str = str(e);
        if "overloaded" in error_str or "503" in error_str {
            print("ğŸ’¡ This is actually GOOD news!");
            print("âœ… byLLM integration is working correctly");
            print("ğŸ”„ Gemini API is just temporarily overloaded");
            print("â° Try again in a few minutes");
        } elif "API key" in error_str or "authentication" in error_str {
            print("ğŸ”‘ API key issue detected");
            print("ğŸ’¡ Run: python setup_gemini_api.py");
        } else {
            print("ğŸ”§ Other API issue - check your configuration");
        }
    }
}